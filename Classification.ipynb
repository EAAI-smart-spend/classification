{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY-Pft9NTo6P"
   },
   "source": [
    "**Run Expense Categorization Model (SVM, Naive Bayes, Decision Tree, KNN & BERT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsB8GdHeAWfS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19262,
     "status": "ok",
     "timestamp": 1765112675802,
     "user": {
      "displayName": "henry chan",
      "userId": "02080567404318967830"
     },
     "user_tz": -480
    },
    "id": "UB7xMAFwd2nn",
    "outputId": "a935e849-a506-4713-d9e6-4392d3d74253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\": Expected package name at the start of dependency specifier\n",
      "    'accelerate\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expense Categorization: 5-Model Comparison (DT, KNN, SVM, NB, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Weights & Biases logging to prevent login prompt ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'C:/Users/User/Downloads/Classification/Classification/expenses.csv'\n",
    "\n",
    "# try:\n",
    "    # print(f\"Loading {csv_filename}...\")\n",
    "df = pd.read_csv(csv_filename)\n",
    "    # print(\"✅ Data Loaded Successfully!\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"❌ Error: {csv_filename} not found.\")\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "### Vectorization (Needed for all traditional ML models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1600, Testing samples: 400\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TRADITIONAL ML (4 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Traditional Machine Learning Models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Traditional Machine Learning Models...\")\n",
    "ml_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- A. Naive Bayes (NB) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Naive Bayes Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_acc = accuracy_score(y_test, nb_model.predict(X_test_tfidf))\n",
    "ml_results['Naive Bayes'] = nb_acc\n",
    "print(f\"✅ Naive Bayes Accuracy: {nb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- B. Support Vector Machine (SVM) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVM Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "svm_acc = accuracy_score(y_test, svm_model.predict(X_test_tfidf))\n",
    "ml_results['SVM'] = svm_acc\n",
    "print(f\"✅ SVM Accuracy: {svm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- C. Decision Tree (DT) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Decision Tree Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_tfidf, y_train)\n",
    "dt_acc = accuracy_score(y_test, dt_model.predict(X_test_tfidf))\n",
    "ml_results['Decision Tree'] = dt_acc\n",
    "print(f\"✅ Decision Tree Accuracy: {dt_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- D. K-Nearest Neighbors (KNN) ---\n",
    "### Using n_neighbors=3 for a simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KNN Accuracy: 0.9900\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train_tfidf, y_train)\n",
    "knn_acc = accuracy_score(y_test, knn_model.predict(X_test_tfidf))\n",
    "ml_results['KNN'] = knn_acc\n",
    "print(f\"✅ KNN Accuracy: {knn_acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- E. Logistic Regression ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logistic Regression Accuracy: 1.0000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=1000, C=5)\n",
    "\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "logistic_acc = accuracy_score(y_test, logistic_model.predict(X_test_tfidf))\n",
    "ml_results['Logistic Regression'] = logistic_acc\n",
    "\n",
    "print(f\"✅ Logistic Regression Accuracy: {logistic_acc:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DEEP LEARNING (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Deep Learning (BERT)...\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT model (No Login Required)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m\n\u001b[0;32m     42\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     44\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score(p\u001b[38;5;241m.\u001b[39mlabel_ids, p\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))}\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining BERT model (No Login Required)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     50\u001b[0m bert_eval \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     51\u001b[0m bert_acc \u001b[38;5;241m=\u001b[39m bert_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2326\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2327\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2328\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2329\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2330\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4026\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1482\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1482\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1483\u001b[0m     input_ids,\n\u001b[0;32m   1484\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1485\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1486\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1487\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1488\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1489\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1490\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1491\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1492\u001b[0m )\n\u001b[0;32m   1494\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1496\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1000\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1000\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1001\u001b[0m     embedding_output,\n\u001b[0;32m   1002\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1003\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1004\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1005\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1006\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1007\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1008\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1009\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1010\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1011\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[0;32m   1013\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1014\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:650\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    646\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 650\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    651\u001b[0m     hidden_states,\n\u001b[0;32m    652\u001b[0m     attention_mask,\n\u001b[0;32m    653\u001b[0m     layer_head_mask,\n\u001b[0;32m    654\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    656\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    657\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    658\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:588\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    585\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    586\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    590\u001b[0m )\n\u001b[0;32m    591\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:596\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 596\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m    597\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:512\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 512\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    513\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Initializing Deep Learning (BERT)...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "model.to(device)\n",
    "\n",
    "class ExpenseDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ExpenseDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "test_dataset = ExpenseDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', num_train_epochs=3, per_device_train_batch_size=8,\n",
    "    logging_dir='./logs', logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n",
    ")\n",
    "\n",
    "print(\"Training BERT model (No Login Required)...\")\n",
    "trainer.train()\n",
    "\n",
    "bert_eval = trainer.evaluate()\n",
    "bert_acc = bert_eval['eval_accuracy']\n",
    "ml_results['BERT'] = bert_acc\n",
    "print(f\"✅ BERT Accuracy: {bert_acc:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 4. PLOTTING COMPARISON\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5-Model Comparison Plot...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Generating 5-Model Comparison Plot...\")\n",
    "models = list(ml_results.keys())\n",
    "accuracies = list(ml_results.values())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "# Use a color palette to distinguish ML types\n",
    "colors = ['#A8DADC', '#457B9D', '#1D3557', '#F4A261', '#E76F51']\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Comparison of 5 NLP Classification Models')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.2f}\",\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 5. PREDICTION FUNCTION\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 74734,
     "status": "ok",
     "timestamp": 1765114854989,
     "user": {
      "displayName": "henry chan",
      "userId": "02080567404318967830"
     },
     "user_tz": -480
    },
    "id": "os3vi-MVAk_H",
    "outputId": "52b10629-7df4-4302-fbe9-898b67027bff"
   },
   "outputs": [],
   "source": [
    "\n",
    "CATEGORY_MAP = {\n",
    "    0: \"Groceries (雜貨/超市購物)\", 1: \"Transportation (交通)\",\n",
    "    2: \"Utilities (公用事業)\", 3: \"Entertainment (娛樂)\",\n",
    "    4: \"Food & Drinks (食物與飲料)\"\n",
    "}\n",
    "\n",
    "def predict_expense(text_input):\n",
    "    \"\"\"Predicts expense category using all five models.\"\"\"\n",
    "    global nb_model, svm_model, dt_model, knn_model, model, vectorizer, tokenizer, device, CATEGORY_MAP\n",
    "\n",
    "    # --- Vectorization ---\n",
    "    vec = vectorizer.transform([text_input])\n",
    "\n",
    "    # --- ML Predictions ---\n",
    "    nb_p = nb_model.predict(vec)[0]\n",
    "    svm_p = svm_model.predict(vec)[0]\n",
    "    dt_p = dt_model.predict(vec)[0]\n",
    "    knn_p = knn_model.predict(vec)[0]\n",
    "\n",
    "    # --- DL Prediction (BERT) ---\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=64).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    bert_p = logits.argmax().item()\n",
    "\n",
    "    print(f\"\\n--- Prediction for: '{text_input}' ---\")\n",
    "    print(f\"**Naive Bayes:** [{nb_p}] {CATEGORY_MAP[nb_p]}\")\n",
    "    print(f\"**SVM:** [{svm_p}] {CATEGORY_MAP[svm_p]}\")\n",
    "    print(f\"**Decision Tree:** [{dt_p}] {CATEGORY_MAP[dt_p]}\")\n",
    "    print(f\"**KNN:** [{knn_p}] {CATEGORY_MAP[knn_p]}\")\n",
    "    print(f\"**BERT:** [{bert_p}] {CATEGORY_MAP[bert_p]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1765114864023,
     "user": {
      "displayName": "henry chan",
      "userId": "02080567404318967830"
     },
     "user_tz": -480
    },
    "id": "EY-ZujUX9gUw",
    "outputId": "e3d90cd1-3b53-46af-f31f-13d285b48bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Predictions on Test Inputs ---\n",
      "\n",
      "--- Prediction for: 'Sushi and beer with friends HKD120' ---\n",
      "**Naive Bayes:** [4] Food & Drinks (食物與飲料)\n",
      "**SVM:** [4] Food & Drinks (食物與飲料)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [4] Food & Drinks (食物與飲料)\n",
      "**BERT:** [4] Food & Drinks (食物與飲料)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: 'Uber $110.50' ---\n",
      "**Naive Bayes:** [1] Transportation (交通)\n",
      "**SVM:** [1] Transportation (交通)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [2] Utilities (公用事業)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: 'Electric bill' ---\n",
      "**Naive Bayes:** [2] Utilities (公用事業)\n",
      "**SVM:** [2] Utilities (公用事業)\n",
      "**Decision Tree:** [2] Utilities (公用事業)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [2] Utilities (公用事業)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: '咖哩牛腩飯' ---\n",
      "**Naive Bayes:** [0] Groceries (雜貨/超市購物)\n",
      "**SVM:** [1] Transportation (交通)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [4] Food & Drinks (食物與飲料)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: '米線' ---\n",
      "**Naive Bayes:** [0] Groceries (雜貨/超市購物)\n",
      "**SVM:** [1] Transportation (交通)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [2] Utilities (公用事業)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: 'Concert tickets for VIP seat USD900' ---\n",
      "**Naive Bayes:** [3] Entertainment (娛樂)\n",
      "**SVM:** [3] Entertainment (娛樂)\n",
      "**Decision Tree:** [3] Entertainment (娛樂)\n",
      "**KNN:** [3] Entertainment (娛樂)\n",
      "**BERT:** [3] Entertainment (娛樂)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: 'Socks' ---\n",
      "**Naive Bayes:** [0] Groceries (雜貨/超市購物)\n",
      "**SVM:** [1] Transportation (交通)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [4] Food & Drinks (食物與飲料)\n",
      "----------------------------------------\n",
      "\n",
      "--- Prediction for: '恐龍毛公仔' ---\n",
      "**Naive Bayes:** [0] Groceries (雜貨/超市購物)\n",
      "**SVM:** [1] Transportation (交通)\n",
      "**Decision Tree:** [4] Food & Drinks (食物與飲料)\n",
      "**KNN:** [2] Utilities (公用事業)\n",
      "**BERT:** [4] Food & Drinks (食物與飲料)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with Examples\n",
    "print(\"--- Running Predictions on Test Inputs ---\")\n",
    "predict_expense('Sushi and beer with friends HKD120')\n",
    "predict_expense('Uber $110.50')\n",
    "predict_expense('Electric bill')\n",
    "predict_expense('咖哩牛腩飯')\n",
    "predict_expense('米線')\n",
    "predict_expense('Concert tickets for VIP seat USD900')\n",
    "predict_expense('Socks')\n",
    "predict_expense('恐龍毛公仔')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
